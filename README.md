![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)![Google Colab](https://img.shields.io/badge/Google%20Colab-%23F9A825.svg?style=for-the-badge&logo=googlecolab&logoColor=white)![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white)![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=for-the-badge&logo=Matplotlib&logoColor=black)

# BipedalWalker-v3 with DDPQ and PPO

В данном коде реализованы 2 метода в обучении с подкреплением (**RL**), а именно: **DDPQ** и **PPO**. 

## DDPG (Deep Deterministic Policy Gradient):

**Принцип работы: DDPG**  — это алгоритм, который использует детерминированную политику для работы с непрерывными пространствами действий. Он сочетает в себе идеи Q-обучения и градиента политики.

**Особенности**:

-   **Детерминированная политика:**  Прямое отображение состояний в действия.
    
-   **Исследование через добавление шума:** Добавление стохастического шума к действиям для стимулирования исследования.
    
-   **Буфер воспроизведения:** Хранение опыта в буфере и повторное использование для обучения.

## PPO (Proximal Policy Optimization):

**Принцип работы: PPO** — это алгоритм, который ограничивает обновления политики, чтобы предотвратить резкие изменения и обеспечить стабильность обучения.

**Особенности**:

-   **Ограничение обновлений политики:**  Использование ограниченного обновления политики для стабилизации обучения.
    
-   **Возможность повторного использования выборок:**  Повторное использование выборок для улучшения эффективности обучения.
    
-   **Стабильная оптимизация:**  Обеспечение стабильной оптимизации политики.

Выполнен анализ полученных результатов, в котором происходит сравнение работы 2 методов на базе такой среды, как **BipedalWalker-v3**. Написаны вывод и рекомендации по улучшению модели.

С более подробным описанием кода и блоков кода можно ознакомиться в **Google Colab Notebook**, представленном в данном репозитории.

> Настоятельно рекомендую использовать **графический ускоритель T4** из
> **Google Colab** для запуска этого кода!
